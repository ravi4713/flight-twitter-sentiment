# -*- coding: utf-8 -*-
"""0000000017643048_notebook_PROJECT-TWITTERSENTIMENTANALYSIS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q3cBtMkdiNIxATNOfaeO-R0fkwrTSf3g
"""

import numpy as np,pandas as pd,string
from nltk.corpus import wordnet,stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize,word_tokenize
from nltk import pos_tag

lemmatizer = WordNetLemmatizer()

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

stop = stopwords.words('english')
punc = list(string.punctuation)
stop = stop + punc
stop

data_train = pd.read_csv("twitter_train.csv")
data_test = pd.read_csv("twitter_test.csv")

data_train.head()

dat_train = data_train.loc[:,"text"]
dat_test = data_test.loc[:,"text"]
cat_train = data_train.loc[:,"airline_sentiment"]

def get_simple_pos_tag(tag):
    if tag[0]=='J':
        return wordnet.ADJ
    elif tag[0]=='V':
        return wordnet.VERB
    elif tag[0]=='N':
        return wordnet.NOUN
    elif tag[0]=='R':
        return wordnet.ADV
    else:
        return wordnet.NOUN

def clean_reviews(words):
    output_words = []
    for w in words:
        if w.lower() not in stop:
            ps = pos_tag([w])
            clean_word = lemmatizer.lemmatize(w,pos=get_simple_pos_tag(ps[0][1]))
            output_words.append(clean_word.lower())
    return output_words

documents_test = []
for i in range(len(dat_test)):
    documents_test.append(dat_test[i])
documents_test

documents = []
for i in range(len(dat_train)):
    documents.append(dat_train[i])
documents

docum = [list(word_tokenize(doc)) for doc in documents]

docum_test = [list(word_tokenize(doc)) for doc in documents_test]

docum = [clean_reviews(doc) for doc in docum]

docum_test = [clean_reviews(doc) for doc in docum_test]

docum = [" ".join(doc) for doc in docum]
docum_test = [" ".join(doc) for doc in docum_test]

from sklearn.feature_extraction.text import CountVectorizer

cat_train = list(cat_train)

count_vec = CountVectorizer(max_features = 2000,stop_words=stop,ngram_range=(1,2))
x_train_features = count_vec.fit_transform(docum,cat_train)
x_test_features = count_vec.transform(docum_test)

joblib.dump(count_vec, open("count_vec.pkl", "wb"))

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_train_features, cat_train, random_state=2)



from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()

rfc.fit(x_train,y_train)



from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB() # making the classifier object
clf.fit(x_train,y_train)

from sklearn.linear_model import LogisticRegression
clflg = LogisticRegression(multi_class="ovr",solver="lbfgs",max_iter=300) # making the classifier object
clflg.fit(x_train,y_train)

from sklearn.neighbors import KNeighborsClassifier
clfknn = KNeighborsClassifier() # making the classifier object
clfknn.fit(x_train,y_train)

scores_train = []
scores_test = []
name = []

name.append('RF')
scores_train.append(rfc.score(x_train, y_train))
scores_test.append(rfc.score(x_test, y_test))

name.append('NB')
scores_train.append(clf.score(x_train, y_train))
scores_test.append(clf.score(x_test, y_test))

name.append('LG')
scores_train.append(clflg.score(x_train, y_train))
scores_test.append(clflg.score(x_test, y_test))

name.append('KNN')
scores_train.append(clfknn.score(x_train, y_train))
scores_test.append(clfknn.score(x_test, y_test))

import matplotlib.pyplot as plt

plt.plot(name, scores_train)
plt.title('Training score')
print(scores_train)


plt.plot(name, scores_test)
plt.title('Testing score')
print(scores_test)

plt.show()

from sklearn.externals import joblib

joblib.dump(rfc, 'rf.pkl')
joblib.dump(clf, 'nb.pkl')
joblib.dump(clflg, 'lg.pkl')

